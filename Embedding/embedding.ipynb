{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- define tiny corpus\n",
    "- lowercase + whitespace tokenize\n",
    "- build vocabulary and mappings:\n",
    "    - stoi (string -> int ID)\n",
    "    - itos (int ID -> string)\n",
    "    - these IDs are exactly what nn.Embedding will lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      "[['we', 'all', 'live', 'in', 'a', 'yellow', 'submarine'], ['we', 'all', 'live', 'in', 'a', 'blue', 'submarine'], ['we', 'all', 'love', 'bright', 'yellow', 'flowers'], ['they', 'all', 'live', 'in', 'a', 'green', 'house'], ['they', 'all', 'love', 'blue', 'flowers']]\n",
      "\n",
      "all tokens:\n",
      "['we', 'all', 'live', 'in', 'a', 'yellow', 'submarine', 'we', 'all', 'live', 'in', 'a', 'blue', 'submarine', 'we', 'all', 'love', 'bright', 'yellow', 'flowers', 'they', 'all', 'live', 'in', 'a', 'green', 'house', 'they', 'all', 'love', 'blue', 'flowers']\n",
      "\n",
      "sorted unique tokens (vocab):\n",
      "['a', 'all', 'blue', 'bright', 'flowers', 'green', 'house', 'in', 'live', 'love', 'submarine', 'they', 'we', 'yellow']\n",
      "\n",
      "string to id mapping:\n",
      "{'a': 0, 'all': 1, 'blue': 2, 'bright': 3, 'flowers': 4, 'green': 5, 'house': 6, 'in': 7, 'live': 8, 'love': 9, 'submarine': 10, 'they': 11, 'we': 12, 'yellow': 13}\n",
      "\n",
      "id to string mapping:\n",
      "{0: 'a', 1: 'all', 2: 'blue', 3: 'bright', 4: 'flowers', 5: 'green', 6: 'house', 7: 'in', 8: 'live', 9: 'love', 10: 'submarine', 11: 'they', 12: 'we', 13: 'yellow'}\n",
      "\n",
      "vocab length (V):\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "corpus = [\n",
    "    \"we all live in a yellow submarine\",\n",
    "    \"we all live in a blue submarine\",\n",
    "    \"we all love bright yellow flowers\",\n",
    "    \"they all live in a green house\",\n",
    "    \"they all love blue flowers\",\n",
    "]\n",
    "\n",
    "# simple whitespace tokenizer\n",
    "def tokenize(s):\n",
    "    return s.lower().split()\n",
    "\n",
    "tokens = [tokenize(s) for s in corpus]\n",
    "print(f\"tokens:\\n{tokens}\\n\")\n",
    "\n",
    "all_tokens = list(chain.from_iterable(tokens))\n",
    "print(f\"all tokens:\\n{all_tokens}\\n\")\n",
    "\n",
    "# unique tokens sorted for reproducibility\n",
    "vocab = sorted(Counter(all_tokens).keys())\n",
    "print(f\"sorted unique tokens (vocab):\\n{vocab}\\n\")\n",
    "\n",
    "# string -> id\n",
    "stoi = {w: i for i, w in enumerate(vocab)}\n",
    "print(f\"string to id mapping:\\n{stoi}\\n\")\n",
    "\n",
    "# id -> string\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "print(f\"id to string mapping:\\n{itos}\\n\")\n",
    "\n",
    "V = len(vocab)\n",
    "print(f\"vocab length (V):\\n{V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding as one_hot x table and as row lookup\n",
    "one-hot encoding - a way to represents categorical values (like words) as numeric vectors\n",
    "- e.g. V = 5\n",
    "    - 0: we\n",
    "    - 1: all\n",
    "    - 2: live\n",
    "    - 3: yellow\n",
    "    - 4: flowers\n",
    "- yellow (ID 3) would be represented as one_hot = [0, 0, 0, 1, 0]\n",
    "    - a vector of length V\n",
    "    - all zeros except a 1 at the index of the word's ID\n",
    "\n",
    "one-hot x table\n",
    "- e.g. embedding table (V, D) where V = 5 and D = 3\n",
    "```\n",
    "W =\n",
    "[ 0.2,  0.5, -0.1 ]   # row 0 -> \"we\"\n",
    "[-0.4,  0.1,  0.8 ]   # row 1 -> \"all\"\n",
    "[ 0.9, -0.7,  0.3 ]   # row 2 -> \"live\"\n",
    "[ 0.0,  1.2, -0.6 ]   # row 3 -> \"yellow\"\n",
    "[-0.2,  0.3,  0.5 ]   # row 4 -> \"flowers\"\n",
    "```\n",
    "- one_hot x W = [0, 0, 0, 1, 0] x W = [0.0, 1.2, -0.6]\n",
    "    the enbedding vecotr for yellow\n",
    "\n",
    "- So, the embedding vector for a token ID is just:\n",
    "    - the row of a (V, D) table\n",
    "    - equivalently, a matrix multiply of a one-hot vector with that table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: yellow | ID: 13\n",
      "Table shape (V, D): (14, 6)\n",
      "via_mm shape: torch.Size([6]) | via_row shape: torch.Size([6])\n",
      "Equal (allclose)? True\n",
      "\n",
      "Embedding vector (first 3 dims):\n",
      "via_mm : tensor([ 0.0335,  0.7101, -1.5353])\n",
      "via_row: tensor([ 0.0335,  0.7101, -1.5353])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "D = 6\n",
    "W = torch.randn(V, D)\n",
    "\n",
    "def one_hot(index, size):\n",
    "    v = torch.zeros(size)\n",
    "    v[index] = 1.0\n",
    "    return v\n",
    "\n",
    "token = \"yellow\" if \"yellow\" in stoi else list(stoi.keys())[0]\n",
    "tid = stoi[token]\n",
    "\n",
    "# create one-hot vectro\n",
    "oh = one_hot(tid, V)    # (V,)\n",
    "\n",
    "# method 1 via matrix multiplication\n",
    "via_mm = oh @ W         # (D,): one-hot times table\n",
    "# method 2 via row lookup\n",
    "via_row = W[tid]        # (D,): direct row gather\n",
    "\n",
    "print(\"Token:\", token, \"| ID:\", tid)\n",
    "print(\"Table shape (V, D):\", tuple(W.shape))\n",
    "print(\"via_mm shape:\", via_mm.shape, \"| via_row shape:\", via_row.shape)\n",
    "\n",
    "# check numerical equality\n",
    "print(\"Equal (allclose)?\", torch.allclose(via_mm, via_row))\n",
    "\n",
    "# peek at the vector\n",
    "print(\"\\nEmbedding vector (first 3 dims):\")\n",
    "print(\"via_mm :\", via_mm[:3])\n",
    "print(\"via_row:\", via_row[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement MyEmbedding and compare to nn.Embedding\n",
    "- a custom embedding layer is just:\n",
    "    - a trainable table (V, D)\n",
    "    - a row gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes equal?  True\n",
      "Values allclose?  True\n",
      "Output (first row, first 3 dims):\n",
      "MyEmbedding: tensor([ 0.5588,  0.7918, -0.1847], grad_fn=<SliceBackward0>)\n",
      "nn.Embedding: tensor([ 0.5588,  0.7918, -0.1847], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        return self.weight[ids]\n",
    "\n",
    "# reuse V, choose embedding dim\n",
    "D = 6\n",
    "\n",
    "my_emb = MyEmbedding(V, D)      # my embedding\n",
    "pt_emb = nn.Embedding(V, D)     # PyTorch embedding\n",
    "\n",
    "# for fair comparison, make them start from the same weights\n",
    "with torch.no_grad():\n",
    "    my_emb.weight.copy_(pt_emb.weight)\n",
    "\n",
    "example_ids = torch.tensor([stoi.get(\"we\", 0), stoi.get(\"yellow\", 0), stoi.get(\"flowers\", 0)])\n",
    "\n",
    "# Python calls nn.Module.__call__ which calls the forward method\n",
    "out_my = my_emb(example_ids)\n",
    "out_pt = pt_emb(example_ids)\n",
    "\n",
    "print(\"Shapes equal? \", out_my.shape == out_pt.shape)\n",
    "print(\"Values allclose? \", torch.allclose(out_my, out_pt))\n",
    "print(\"Output (first row, first 3 dims):\")\n",
    "print(\"MyEmbedding:\", out_my[0, :3])\n",
    "print(\"nn.Embedding:\", out_pt[0, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only used rows get gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero grad rows in MyEmbedding:\n",
      "[4, 12, 13]\n",
      "Non-zero grad rows in nn.Embedding:\n",
      "[4, 12, 13]\n",
      "Unique example IDs: [4, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "# make a simple scalar loss: sum of output vectors -> backward\n",
    "my_emb.zero_grad(set_to_none=True)\n",
    "pt_emb.zero_grad(set_to_none=True)\n",
    "\n",
    "# my embedding\n",
    "out_my = my_emb(example_ids)    # (3, 6)\n",
    "loss_my = out_my.sum()          # sums over all elements          \n",
    "loss_my.backward()\n",
    "\n",
    "print(\"Non-zero grad rows in MyEmbedding:\")\n",
    "nz_my = (my_emb.weight.grad.abs().sum(dim=1) > 0).nonzero(as_tuple=True)[0]\n",
    "print(nz_my.tolist())\n",
    "\n",
    "# PyTorch embedding\n",
    "out_pt = pt_emb(example_ids)    # (3, 6)\n",
    "loss_pt = out_pt.sum()          # sums over all elements\n",
    "loss_pt.backward()\n",
    "\n",
    "print(\"Non-zero grad rows in nn.Embedding:\")\n",
    "nz_pt = (pt_emb.weight.grad.abs().sum(dim=1) > 0).nonzero(as_tuple=True)[0]\n",
    "print(nz_pt.tolist())\n",
    "\n",
    "# The sets should match the unique IDs we looked up\n",
    "print(\"Unique example IDs:\", torch.unique(example_ids).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a minimal context to next-word model\n",
    "next-word predictor\n",
    "- input: average of embedding of a 2-word context\n",
    "- output: distribution over vocab for the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids from curr line: [12, 1, 8, 7, 0, 13, 10]\n",
      "ids from curr line: [12, 1, 8, 7, 0, 2, 10]\n",
      "ids from curr line: [12, 1, 9, 3, 13, 4]\n",
      "ids from curr line: [11, 1, 8, 7, 0, 5, 6]\n",
      "ids from curr line: [11, 1, 9, 2, 4]\n",
      "tokens: [['we', 'all', 'live', 'in', 'a', 'yellow', 'submarine'], ['we', 'all', 'live', 'in', 'a', 'blue', 'submarine'], ['we', 'all', 'love', 'bright', 'yellow', 'flowers'], ['they', 'all', 'live', 'in', 'a', 'green', 'house'], ['they', 'all', 'love', 'blue', 'flowers']]\n",
      "pairs: [([12, 1], 8), ([1, 8], 7), ([8, 7], 0), ([7, 0], 13), ([0, 13], 10), ([12, 1], 8), ([1, 8], 7), ([8, 7], 0), ([7, 0], 2), ([0, 2], 10), ([12, 1], 9), ([1, 9], 3), ([9, 3], 13), ([3, 13], 4), ([11, 1], 8), ([1, 8], 7), ([8, 7], 0), ([7, 0], 5), ([0, 5], 6), ([11, 1], 9), ([1, 9], 2), ([9, 2], 4)]\n"
     ]
    }
   ],
   "source": [
    "# build context to target pairs (window size = 2, predict next token)\n",
    "def make_pairs(token_lines, window=2):\n",
    "    pairs = []\n",
    "    for line in token_lines:\n",
    "        # get ids from each line\n",
    "        ids = [stoi[w] for w in line]\n",
    "        print(f'ids from curr line: {ids}')\n",
    "        for i in range(len(ids) - window):\n",
    "            ctx = ids[i:i+window]   # 2-word context\n",
    "            tgt = ids[i+window]     # next word\n",
    "            pairs.append((ctx, tgt))\n",
    "    return pairs\n",
    "\n",
    "pairs = make_pairs(tokens, window=2)\n",
    "print(f'tokens: {tokens}')\n",
    "print(f'pairs: {pairs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model = Embedding -> mean -> Linear -> softmax\n",
    "class CtxAvgNextWord(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.proj = nn.Linear(emb_dim, vocab_size)\n",
    "    def forward(self, ctx_ids):\n",
    "        E = self.emb(ctx_ids)\n",
    "        h = E.mean(dim=1)\n",
    "        logits = self.proj(h)\n",
    "        return logits\n",
    "\n",
    "# initialize model and optimizer\n",
    "D = 6\n",
    "model = CtxAvgNextWord(V, D)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Avg Loss: 0.377\n",
      "Epoch 100 | Avg Loss: 0.373\n",
      "Epoch 150 | Avg Loss: 0.371\n",
      "Epoch 200 | Avg Loss: 0.373\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# make mini-batches from the (context, target) pairs\n",
    "def batch_iter(pairs, batch__size):\n",
    "    random.shuffle(pairs)\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        chunk = pairs[i:i+batch_size]\n",
    "        ctx = torch.tensor([c for c, t in chunk], dtype=torch.long) # (B, 2)\n",
    "        tgt = torch.tensor([t for c, t in chunk], dtype=torch.long) # (B,)\n",
    "        yield ctx, tgt\n",
    "\n",
    "# train the model\n",
    "epochs = 200\n",
    "batch_size = 8\n",
    "for epoch in range(1, epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    for ctx, tgt in batch_iter(pairs, batch_size):\n",
    "        logits = model(ctx)                 # (B, V)\n",
    "        loss = F.cross_entropy(logits, tgt) # compares to true target IDs\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item() * ctx.size(0) # for weighted average\n",
    "        n += ctx.size(0)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Avg Loss: {total_loss/n:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
