{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- define tiny corpus\n",
    "- lowercase + whitespace tokenize\n",
    "- build vocabulary and mappings:\n",
    "    - stoi (string -> int ID)\n",
    "    - itos (int ID -> string)\n",
    "    - these IDs are exactly what nn.Embedding will lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      "[['we', 'all', 'live', 'in', 'a', 'yellow', 'submarine'], ['we', 'all', 'live', 'in', 'a', 'blue', 'submarine'], ['we', 'all', 'love', 'bright', 'yellow', 'flowers'], ['they', 'all', 'live', 'in', 'a', 'green', 'house'], ['they', 'all', 'love', 'blue', 'flowers']]\n",
      "\n",
      "all tokens:\n",
      "['we', 'all', 'live', 'in', 'a', 'yellow', 'submarine', 'we', 'all', 'live', 'in', 'a', 'blue', 'submarine', 'we', 'all', 'love', 'bright', 'yellow', 'flowers', 'they', 'all', 'live', 'in', 'a', 'green', 'house', 'they', 'all', 'love', 'blue', 'flowers']\n",
      "\n",
      "sorted unique tokens (vocab):\n",
      "['a', 'all', 'blue', 'bright', 'flowers', 'green', 'house', 'in', 'live', 'love', 'submarine', 'they', 'we', 'yellow']\n",
      "\n",
      "string to id mapping:\n",
      "{'a': 0, 'all': 1, 'blue': 2, 'bright': 3, 'flowers': 4, 'green': 5, 'house': 6, 'in': 7, 'live': 8, 'love': 9, 'submarine': 10, 'they': 11, 'we': 12, 'yellow': 13}\n",
      "\n",
      "id to string mapping:\n",
      "{0: 'a', 1: 'all', 2: 'blue', 3: 'bright', 4: 'flowers', 5: 'green', 6: 'house', 7: 'in', 8: 'live', 9: 'love', 10: 'submarine', 11: 'they', 12: 'we', 13: 'yellow'}\n",
      "\n",
      "vocab length (V):\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "corpus = [\n",
    "    \"we all live in a yellow submarine\",\n",
    "    \"we all live in a blue submarine\",\n",
    "    \"we all love bright yellow flowers\",\n",
    "    \"they all live in a green house\",\n",
    "    \"they all love blue flowers\",\n",
    "]\n",
    "\n",
    "# simple whitespace tokenizer\n",
    "def tokenize(s):\n",
    "    return s.lower().split()\n",
    "\n",
    "tokens = [tokenize(s) for s in corpus]\n",
    "print(f\"tokens:\\n{tokens}\\n\")\n",
    "\n",
    "all_tokens = list(chain.from_iterable(tokens))\n",
    "print(f\"all tokens:\\n{all_tokens}\\n\")\n",
    "\n",
    "# unique tokens sorted for reproducibility\n",
    "vocab = sorted(Counter(all_tokens).keys())\n",
    "print(f\"sorted unique tokens (vocab):\\n{vocab}\\n\")\n",
    "\n",
    "# string -> id\n",
    "stoi = {w: i for i, w in enumerate(vocab)}\n",
    "print(f\"string to id mapping:\\n{stoi}\\n\")\n",
    "\n",
    "# id -> string\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "print(f\"id to string mapping:\\n{itos}\\n\")\n",
    "\n",
    "V = len(vocab)\n",
    "print(f\"vocab length (V):\\n{V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding as one_hot x table and as row lookup\n",
    "one-hot encoding - a way to represents categorical values (like words) as numeric vectors\n",
    "- e.g. V = 5\n",
    "    - 0: we\n",
    "    - 1: all\n",
    "    - 2: live\n",
    "    - 3: yellow\n",
    "    - 4: flowers\n",
    "- yellow (ID 3) would be represented as one_hot = [0, 0, 0, 1, 0]\n",
    "    - a vector of length V\n",
    "    - all zeros except a 1 at the index of the word's ID\n",
    "\n",
    "one-hot x table\n",
    "- e.g. embedding table (V, D) where V = 5 and D = 3\n",
    "```\n",
    "W =\n",
    "[ 0.2,  0.5, -0.1 ]   # row 0 -> \"we\"\n",
    "[-0.4,  0.1,  0.8 ]   # row 1 -> \"all\"\n",
    "[ 0.9, -0.7,  0.3 ]   # row 2 -> \"live\"\n",
    "[ 0.0,  1.2, -0.6 ]   # row 3 -> \"yellow\"\n",
    "[-0.2,  0.3,  0.5 ]   # row 4 -> \"flowers\"\n",
    "```\n",
    "- one_hot x W = [0, 0, 0, 1, 0] x W = [0.0, 1.2, -0.6]\n",
    "    the enbedding vecotr for yellow\n",
    "\n",
    "- So, the embedding vector for a token ID is just:\n",
    "    - the row of a (V, D) table\n",
    "    - equivalently, a matrix multiply of a one-hot vector with that table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: yellow | ID: 13\n",
      "Table shape (V, D): (14, 6)\n",
      "via_mm shape: torch.Size([6]) | via_row shape: torch.Size([6])\n",
      "Equal (allclose)? True\n",
      "\n",
      "Embedding vector (first 3 dims):\n",
      "via_mm : tensor([0.7817, 0.9897, 0.4147])\n",
      "via_row: tensor([0.7817, 0.9897, 0.4147])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "D = 6\n",
    "W = torch.randn(V, D)\n",
    "\n",
    "def one_hot(index, size):\n",
    "    v = torch.zeros(size)\n",
    "    v[index] = 1.0\n",
    "    return v\n",
    "\n",
    "token = \"yellow\" if \"yellow\" in stoi else list(stoi.keys())[0]\n",
    "tid = stoi[token]\n",
    "\n",
    "# create one-hot vectro\n",
    "oh = one_hot(tid, V)    # (V,)\n",
    "\n",
    "# method 1 via matrix multiplication\n",
    "via_mm = oh @ W         # (D,): one-hot times table\n",
    "# method 2 via row lookup\n",
    "via_row = W[tid]        # (D,): direct row gather\n",
    "\n",
    "print(\"Token:\", token, \"| ID:\", tid)\n",
    "print(\"Table shape (V, D):\", tuple(W.shape))\n",
    "print(\"via_mm shape:\", via_mm.shape, \"| via_row shape:\", via_row.shape)\n",
    "\n",
    "# check numerical equality\n",
    "print(\"Equal (allclose)?\", torch.allclose(via_mm, via_row))\n",
    "\n",
    "# peek at the vector\n",
    "print(\"\\nEmbedding vector (first 3 dims):\")\n",
    "print(\"via_mm :\", via_mm[:3])\n",
    "print(\"via_row:\", via_row[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement MyEmbedding and compare to nn.Embedding\n",
    "- a custom embedding layer is just:\n",
    "    - a trainable table (V, D)\n",
    "    - a row gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes equal?  True\n",
      "Values allclose?  True\n",
      "Output (first row, first 3 dims):\n",
      "MyEmbedding: tensor([ 0.5455, -1.5374,  0.3124], grad_fn=<SliceBackward0>)\n",
      "nn.Embedding: tensor([ 0.5455, -1.5374,  0.3124], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        return self.weight[ids]\n",
    "\n",
    "# reuse V, choose embedding dim\n",
    "D = 6\n",
    "\n",
    "my_emb = MyEmbedding(V, D)      # my embedding\n",
    "pt_emb = nn.Embedding(V, D)     # PyTorch embedding\n",
    "\n",
    "# for fair comparison, make them start from the same weights\n",
    "with torch.no_grad():\n",
    "    my_emb.weight.copy_(pt_emb.weight)\n",
    "\n",
    "example_ids = torch.tensor([stoi.get(\"we\", 0), stoi.get(\"yellow\", 0), stoi.get(\"flowers\", 0)])\n",
    "\n",
    "# Python calls nn.Module.__call__ which calls the forward method\n",
    "out_my = my_emb(example_ids)\n",
    "out_pt = pt_emb(example_ids)\n",
    "\n",
    "print(\"Shapes equal? \", out_my.shape == out_pt.shape)\n",
    "print(\"Values allclose? \", torch.allclose(out_my, out_pt))\n",
    "print(\"Output (first row, first 3 dims):\")\n",
    "print(\"MyEmbedding:\", out_my[0, :3])\n",
    "print(\"nn.Embedding:\", out_pt[0, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only used rows get gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero grad rows in MyEmbedding:\n",
      "[4, 12, 13]\n",
      "Non-zero grad rows in nn.Embedding:\n",
      "[4, 12, 13]\n",
      "Unique example IDs: [4, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "# make a simple scalar loss: sum of output vectors -> backward\n",
    "my_emb.zero_grad(set_to_none=True)\n",
    "pt_emb.zero_grad(set_to_none=True)\n",
    "\n",
    "# my embedding\n",
    "out_my = my_emb(example_ids)    # (3, 6)\n",
    "loss_my = out_my.sum()          # sums over all elements          \n",
    "loss_my.backward()\n",
    "\n",
    "print(\"Non-zero grad rows in MyEmbedding:\")\n",
    "nz_my = (my_emb.weight.grad.abs().sum(dim=1) > 0).nonzero(as_tuple=True)[0]\n",
    "print(nz_my.tolist())\n",
    "\n",
    "# PyTorch embedding\n",
    "out_pt = pt_emb(example_ids)    # (3, 6)\n",
    "loss_pt = out_pt.sum()          # sums over all elements\n",
    "loss_pt.backward()\n",
    "\n",
    "print(\"Non-zero grad rows in nn.Embedding:\")\n",
    "nz_pt = (pt_emb.weight.grad.abs().sum(dim=1) > 0).nonzero(as_tuple=True)[0]\n",
    "print(nz_pt.tolist())\n",
    "\n",
    "# The sets should match the unique IDs we looked up\n",
    "print(\"Unique example IDs:\", torch.unique(example_ids).tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
