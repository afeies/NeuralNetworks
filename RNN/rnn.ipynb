{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level RNN for Text Generation\n",
    "\n",
    "Train a small character-level language model on a single file to generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Config\n",
    "- imports, device, seed\n",
    "- simple config dictionary for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)                           # Python's random functions\n",
    "    torch.manual_seed(seed)                     # PyTorch CPU ops\n",
    "    torch.cuda.manual_seed_all(seed)            # PyTorch GPU ops\n",
    "    torch.backends.cudnn.deterministic = True   # cuDNN algortihm choice\n",
    "    torch.backends.cudnn.benchmark = False               # cuDNN auto-tuner\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"data_path\": None,      # path to text file (e.g., \"data/shakespeare.text\"); None uses built-in sample\n",
    "    \"seq_len\": 128,         # sequence length for training (input and target chunks)\n",
    "    \"batch_size\": 128,      # number of sequences per training batch\n",
    "    \"embedding_dim\": 256,   # size of character embedding vectors\n",
    "    \"hidden_dim\": 256,      # size of hidden state in RNN (GRU or LSTM)\n",
    "    \"num_layers\": 1,        # number of stacked RNN layers\n",
    "    \"dropout\": 0.1,         # dropout probability between layers\n",
    "    \"rnn_type\": \"GRU\",      # type of RNN: \"GRU\" or \"LSTM\"\n",
    "    \"num_epochs\": 5,        # number of full passes through the training dataset\n",
    "    \"learning_rate\": 2e-3,  # inital learning rate for the optimizer\n",
    "    \"grad_clip\": 1,         # gradient clipping threshold to prevent exploding gradients\n",
    "    \"log_every\": 100,       # how often (in steps) to print training loss\n",
    "    \"sample_every\": 100,    # how often (in steps) to generate sample text\n",
    "    \"max_generate\": 400,    # number of characters tto generate during sampling\n",
    "    \"temperature\": 0.9,     # sampling temperature (controls randomness in output)\n",
    "    \"top_k\": 40,            # top-k sampling: consider only the top k most probable characters\n",
    "    \"top_p\": 0.9,           # top-p (nucleus) sampling: consider top tokens whose probabilites sum to p\n",
    "    \"val_fraction\": 0.05,   # fraction of the dataset to use for validation\n",
    "    \"overlap_step\": None,   # if set, use overlapping training chunks (e.g, step size = seq_len // 2)\n",
    "    \"save_path\": \"char_rnn_checkpoint.pt\",  # where to save the trained model checkpoint\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "- provide a path to corpus (plain .txt) in config[\"data-path\"]\n",
    "- if not provided, we use a built-in snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "if config[\"data_path\"] and os.path.exists(config[\"data_path\"]):\n",
    "    with open(config[\"data_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    # snippet for testing\n",
    "    text = (\n",
    "        \"ROMEO:\\nBut soft, what light through yonder window breaks?\\n\"\n",
    "        \"It is the east, and Juliet is the sun.\\nArise, fair sun, and kill the envious moon,\\n\"\n",
    "        \"Who is already sick and pale with grief.\\n\\n\"\n",
    "        \"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\\n\"\n",
    "        \"Deny thy father and refuse thy name;\\nOr, if thou wilt not, be but sworn my love,\\n\"\n",
    "        \"And I'll no longer be a Capulet.\\n\"       \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Vocabulary\n",
    "- build stoi and itos\n",
    "    - stoi encodes text into IDs for the model, and itos decodes model outputs back into readable text\n",
    "    - stoi (string-to-index): a dict mapping each character to an integer\n",
    "    - itos (index-to-string): a list mapping each ID back to its character\n",
    "- encode/decode utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharVocab:\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(list(set(text)))     # 45 unique chars\n",
    "        self.itos = chars                   \n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "    \n",
    "    # takes text and return list of ids for each char\n",
    "    def encode(self, s: str) -> List[int]:\n",
    "        return [self.stoi[c] for c in s if c in self.stoi]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        return \"\". join(self.itos[i] for i in ids)\n",
    "\n",
    "vocab = CharVocab(text)\n",
    "vocab_size = len(vocab.itos)    # 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode & Split, Dataset & DataLoader\n",
    "- encode the full text into integer IDs\n",
    "- split into train/val by fraction\n",
    "- create chunked dataset returning (x, y) where y is the next-char targets\n",
    "### Additional Notes\n",
    "for Shakespeare text:\n",
    "- train/val split\n",
    "    - len(text) = 347\n",
    "        - the original text\n",
    "    - len(vocab.encode(text)) = 347\n",
    "        - the list of ids for each char\n",
    "    - n_total = len(data_ids) = 347\n",
    "        - turns the encoding into a tensor\n",
    "    - n_val = 347 * 0.05 = 17\n",
    "\n",
    "- CharChunkDataset\n",
    "    - chunk: one training example (128 consecutive characters)\n",
    "    - input: those 128 characters\n",
    "    - target: the next 128 characters, shifted by one position\n",
    "        - the model learns to predict the next character at each step\n",
    "    - len(train_ds) = train_ds.num_chunks = (330 - 1 - 128) // 128 + 1 = 2\n",
    "    - self.starts\n",
    "        - list of starting indices where each chunk begins\n",
    "        - makes getitem fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode entire corpus\n",
    "data_ids = torch.tensor(vocab.encode(text), dtype=torch.long)\n",
    "\n",
    "# train/val split\n",
    "n_total = len(data_ids)\n",
    "n_val = max(1, int(n_total * config[\"val_fraction\"]))   # at least one token for validation\n",
    "train_ids = data_ids[:-n_val]   # 330 ids\n",
    "val_ids = data_ids[-n_val:]     # 17 ids\n",
    "\n",
    "# splits a long 1D tokenized tensor into (input, target) chunk pairs\n",
    "# each sample: x of length T, y of length T (next-char prediction)\n",
    "class CharChunkDataset(Dataset):\n",
    "    def __init__(self, ids: torch.Tensor, seq_len: int, step: Optional[int] = None):\n",
    "        self.ids = ids      # 1d tensor of all character ids\n",
    "        self.T = seq_len    # chunk length\n",
    "        self.step = step if step is not None else seq_len               # stride: None means non-overlapping\n",
    "        self.num_chunks = (len(ids) - 1 - self.T) // self.step + 1      # number of chunks you can extract\n",
    "        self.starts = [i * self.step for i in range(self.num_chunks)]\n",
    "    \n",
    "    # built-in behavior for len(obj)\n",
    "    def __len__(self):\n",
    "        return self.num_chunks\n",
    "    \n",
    "    # built-in behavior for obj[i]\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.starts[idx]\n",
    "        x = self.ids[s : s + self.T]\n",
    "        y = self.ids[s + 1: s + 1 + self.T]\n",
    "        return x, y\n",
    "\n",
    "train_ds = CharChunkDataset(train_ids, config[\"seq_len\"], config[\"overlap_step\"])\n",
    "val_ds = CharChunkDataset(val_ids, config[\"seq_len\"], config[\"overlap_step\"])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
