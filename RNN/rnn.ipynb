{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level RNN for Text Generation\n",
    "\n",
    "Train a small character-level language model on a single file to generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Config\n",
    "- imports, device, seed\n",
    "- simple config dictionary for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)                           # Python's random functions\n",
    "    torch.manual_seed(seed)                     # PyTorch CPU ops\n",
    "    torch.cuda.manual_seed_all(seed)            # PyTorch GPU ops\n",
    "    torch.backends.cudnn.deterministic = True   # cuDNN algortihm choice\n",
    "    torch.backends.cudnn.benchmark = False               # cuDNN auto-tuner\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"data_path\": \"data/tinyshakespeare.txt\",    # path to text file (e.g., \"data/shakespeare.text\"); None uses built-in sample\n",
    "    \"seq_len\": 128,         # sequence length for training (input and target chunks)\n",
    "    \"batch_size\": 128,      # number of sequences per training batch\n",
    "    \"embedding_dim\": 256,   # size of character embedding vectors\n",
    "    \"hidden_dim\": 256,      # size of hidden state in RNN (GRU or LSTM)\n",
    "    \"num_layers\": 1,        # number of stacked RNN layers\n",
    "    \"dropout\": 0.1,         # dropout probability between layers\n",
    "    \"rnn_type\": \"GRU\",      # type of RNN: \"GRU\" or \"LSTM\"\n",
    "    \"num_epochs\": 5,        # number of full passes through the training dataset\n",
    "    \"learning_rate\": 2e-3,  # inital learning rate for the optimizer\n",
    "    \"grad_clip\": 1,         # gradient clipping threshold to prevent exploding gradients\n",
    "    \"log_every\": 100,       # how often (in steps) to print training loss\n",
    "    \"sample_every\": 100,    # how often (in steps) to generate sample text\n",
    "    \"max_generate\": 400,    # number of characters tto generate during sampling\n",
    "    \"temperature\": 0.9,     # sampling temperature (controls randomness in output)\n",
    "    \"top_k\": 40,            # top-k sampling: consider only the top k most probable characters\n",
    "    \"top_p\": 0.9,           # top-p (nucleus) sampling: consider top tokens whose probabilites sum to p\n",
    "    \"val_fraction\": 0.05,   # fraction of the dataset to use for validation\n",
    "    \"overlap_step\": None,   # if set, use overlapping training chunks (e.g, step size = seq_len // 2)\n",
    "    \"save_path\": \"char_rnn_checkpoint.pt\",  # where to save the trained model checkpoint\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "- provide a path to corpus (plain .txt) in config[\"data-path\"]\n",
    "- if not provided, we use a built-in snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "if config[\"data_path\"] and os.path.exists(config[\"data_path\"]):\n",
    "    with open(config[\"data_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    # snippet for testing\n",
    "    text = (\n",
    "        \"ROMEO:\\nBut soft, what light through yonder window breaks?\\n\"\n",
    "        \"It is the east, and Juliet is the sun.\\nArise, fair sun, and kill the envious moon,\\n\"\n",
    "        \"Who is already sick and pale with grief.\\n\\n\"\n",
    "        \"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\\n\"\n",
    "        \"Deny thy father and refuse thy name;\\nOr, if thou wilt not, be but sworn my love,\\n\"\n",
    "        \"And I'll no longer be a Capulet.\\n\"       \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Vocabulary\n",
    "- build stoi and itos\n",
    "    - stoi encodes text into IDs for the model, and itos decodes model outputs back into readable text\n",
    "    - stoi (string-to-index): a dict mapping each character to an integer\n",
    "    - itos (index-to-string): a list mapping each ID back to its character\n",
    "- encode/decode utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharVocab:\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(list(set(text)))     # 45 unique chars\n",
    "        self.itos = chars                   \n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "    \n",
    "    # takes text and return list of ids for each char\n",
    "    def encode(self, s: str) -> List[int]:\n",
    "        return [self.stoi[c] for c in s if c in self.stoi]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        return \"\". join(self.itos[i] for i in ids)\n",
    "\n",
    "vocab = CharVocab(text)\n",
    "vocab_size = len(vocab.itos)    # 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode & Split, Dataset & DataLoader\n",
    "- encode the full text into integer IDs\n",
    "- split into train/val by fraction\n",
    "- create chunked dataset returning (x, y) where y is the next-char targets\n",
    "### Additional Notes\n",
    "for Shakespeare text:\n",
    "- train/val split\n",
    "    - len(text) = 347\n",
    "        - the original text\n",
    "    - len(vocab.encode(text)) = 347\n",
    "        - the list of ids for each char\n",
    "    - n_total = len(data_ids) = 347\n",
    "        - turns the encoding into a tensor\n",
    "    - n_val = 347 * 0.05 = 17\n",
    "\n",
    "- CharChunkDataset\n",
    "    - chunk: one training example (128 consecutive characters)\n",
    "    - input: those 128 characters\n",
    "    - target: the next 128 characters, shifted by one position\n",
    "        - the model learns to predict the next character at each step\n",
    "    - len(train_ds) = train_ds.num_chunks = (330 - 1 - 128) // 128 + 1 = 2\n",
    "    - self.starts\n",
    "        - list of starting indices where each chunk begins\n",
    "        - makes getitem fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode entire corpus\n",
    "data_ids = torch.tensor(vocab.encode(text), dtype=torch.long)\n",
    "\n",
    "# train/val split\n",
    "n_total = len(data_ids)\n",
    "n_val = max(1, int(n_total * config[\"val_fraction\"]))   # at least one token for validation\n",
    "train_ids = data_ids[:-n_val]   # 330 ids\n",
    "val_ids = data_ids[-n_val:]     # 17 ids\n",
    "\n",
    "# splits a long 1D tokenized tensor into (input, target) chunk pairs\n",
    "# each sample: x of length T, y of length T (next-char prediction)\n",
    "class CharChunkDataset(Dataset):\n",
    "    def __init__(self, ids: torch.Tensor, seq_len: int, step: Optional[int] = None):\n",
    "        self.ids = ids      # 1d tensor of all character ids\n",
    "        self.T = seq_len    # chunk length\n",
    "        self.step = step if step is not None else seq_len               # stride: None means non-overlapping\n",
    "        self.num_chunks = (len(ids) - 1 - self.T) // self.step + 1      # number of chunks you can extract\n",
    "        self.starts = [i * self.step for i in range(self.num_chunks)]\n",
    "    \n",
    "    # built-in behavior for len(obj)\n",
    "    def __len__(self):\n",
    "        return self.num_chunks\n",
    "    \n",
    "    # built-in behavior for obj[i]\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.starts[idx]\n",
    "        x = self.ids[s : s + self.T]\n",
    "        y = self.ids[s + 1: s + 1 + self.T]\n",
    "        return x, y\n",
    "\n",
    "train_ds = CharChunkDataset(train_ids, config[\"seq_len\"], config[\"overlap_step\"])\n",
    "val_ds = CharChunkDataset(val_ids, config[\"seq_len\"], config[\"overlap_step\"])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- embedding -> RNN (GRU/LSTM) -> Linear\n",
    "- batch_first=True so inputs are [B, T]\n",
    "- dropout on embeddings and hidden outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb: int, hidden: int, layers: int, dropout: float, rnn_type: str = \"GRU\"):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, emb)\n",
    "        rnn_cls = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[rnn_type.upper()]\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=emb,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=layers,\n",
    "            dropout=dropout if layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden, vocab_size)\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "        self.layers = layers\n",
    "        self.hidden = hidden\n",
    "    \n",
    "    # x: [batch_size=128, seq_len=128]\n",
    "    # h: \n",
    "    def forward(self, x, h=None):\n",
    "        # input ids -> embeddings\n",
    "        x = self.emb(x)         # x: [batch_size=128, seq_len=128, embedding_dim=256]\n",
    "        # embeddings -> RNN\n",
    "        x, h = self.rnn(x, h)   # x: [batch_size=128, seq_len=128, hidden_dim=256], h: [num_layers=1, batch_size=128, hidden_dim=256] (and c for LSTM)\n",
    "        # dropout for regularization\n",
    "        x = self.drop(x)\n",
    "        # projection to logits\n",
    "        # linear layer maps each hidden_dim vector to vocab_size classes\n",
    "        logits = self.fc(x)     # [batch_size=128, seq_len=128, vocab_size=45]\n",
    "        return logits, h\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            h = torch.zeros(self.layers, batch_size, self.hidden, device=device)\n",
    "            c = torch.zeros(self.layers, batch_size, self.hidden, device=device)\n",
    "            return (h, c)\n",
    "        else:\n",
    "            h = torch.zeros(self.layers, batch_size, self.hidden, device=device)\n",
    "            return h\n",
    "\n",
    "model = CharRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    emb=config[\"embedding_dim\"],\n",
    "    hidden=config[\"hidden_dim\"],\n",
    "    layers=config[\"num_layers\"],\n",
    "    dropout=config[\"dropout\"],\n",
    "    rnn_type=config[\"rnn_type\"],\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities\n",
    "- cross-entropy loss on next-char targets\n",
    "- adam optimizer, gradient clipping\n",
    "- bits-per-character (BPC) = CE / ln(2)\n",
    "    - how many bits of information the model needs to encode each character in the text\n",
    "        - lower BPC = better prediction\n",
    "            - BPC = 1 means model needs 1 bit to choose next char\n",
    "            - BPC = log2(V) means model is guessing randomly from V characters\n",
    "    - bpc_from_loss\n",
    "        - nn.CrossEntropyLoss returns negative log likelihood in nats (natural log base e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "def bpc_from_loss(loss_val: float) -> float:\n",
    "    return loss_val / math.log(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text(\n",
    "        model: nn.Module,\n",
    "        vocab: CharVocab,\n",
    "        max_new_tokens: int = 300,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        prompt: str = \"\",\n",
    "        device: str = \"cpu\"\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # if no prompt is provided, start from a random single char\n",
    "        if not prompt:\n",
    "            prompt = random.choice(vocab.itos)\n",
    "        \n",
    "        input_ids = torch.tensor(vocab.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)    # [1, T]\n",
    "        # start with no hidden state and a list of generated chars initialized with the prompt\n",
    "        h = None\n",
    "        generated = list(prompt)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, h = model(input_ids, h)     # logits: [1, T, V]\n",
    "            last_logits = logits[0, -1, :] / max(1e-6, temperature)\n",
    "        \n",
    "        # convert logits to probabilities\n",
    "        probs = torch.softmax(last_logits, dim=-1)\n",
    "\n",
    "        # top-k / top-p filtering\n",
    "        if top_k is not None:\n",
    "            topk_vals, topk_idx = torch.topk(probs, k=min(top_k, probs.size(-1)))\n",
    "            filtered = torch.zeros_like(probs).scatter_(0, topk_idx, topk_vals)\n",
    "            probs = filtered / filtered.sum()\n",
    "        \n",
    "        if top_p is not None:\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "            cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "            mask = cumsum - sorted_probs > top_p\n",
    "            sorted_probs[mask] = 0.0\n",
    "            sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "            idx_choice = torch.multinomial(sorted_probs, num_samples=1)\n",
    "            next_id = sorted_idx[idx_choice]\n",
    "        else:\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        next_id = sorted_idx[idx_choice]\n",
    "        generated.append(vocab.itos[next_id])\n",
    "\n",
    "        input_ids = torch.tensor([next_id], device=device).view(1, 1)\n",
    "    \n",
    "    return \"\".join(generated)\n",
    "\n",
    "def evaluate_loss(data_loader: DataLoader) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss, count = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "    print(count)\n",
    "    if count == 0:\n",
    "        print(\"[warn] evaluate_loss: dataloader is empty (no batches).\")\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    avg = total_loss / max(1, count)\n",
    "    return avg, bpc_from_loss(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "- iterate over batches of (x, y)\n",
    "- zero grad -> forward -> CE loss -> clip -> step\n",
    "- log train loss & bpc\n",
    "- periodically sample text to track progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_ids): 1059624\n",
      "len(val_ids):   55769\n",
      "len(train_ds):  8278\n",
      "len(val_ds):    435\n",
      "len(train_loader): 64\n",
      "len(val_loader):   3\n",
      "3\n",
      "[Validation] Epoch 1: loss 1.5510 | bpc 2.238\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 02 | Step 000100 | train loss 0.5148 | bpc 0.743\n",
      "Sample\n",
      "3\n",
      "[Validation] Epoch 2: loss 1.5463 | bpc 2.231\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "3\n",
      "[Validation] Epoch 3: loss 1.5399 | bpc 2.222\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 04 | Step 000200 | train loss 0.1126 | bpc 0.163\n",
      "Sample\n",
      "3\n",
      "[Validation] Epoch 4: loss 1.5391 | bpc 2.220\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 05 | Step 000300 | train loss 0.6178 | bpc 0.891\n",
      "Sample\n",
      "3\n",
      "[Validation] Epoch 5: loss 1.5359 | bpc 2.216\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "print(\"len(train_ids):\", len(train_ids))\n",
    "print(\"len(val_ids):  \", len(val_ids))\n",
    "print(\"len(train_ds): \", len(train_ds))\n",
    "print(\"len(val_ds):   \", len(val_ds))\n",
    "print(\"len(train_loader):\", len(train_loader))\n",
    "print(\"len(val_loader):  \", len(val_loader))\n",
    "\n",
    "for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "    model.train()\n",
    "    running = 0.0   # running sum of training loss for loggin averages\n",
    "\n",
    "    # iterate over batches of (x, y) where:\n",
    "    # x: [B, T] token ids (input characters)\n",
    "    # y: [B, T] token ids (next characters, shifted by 1)\n",
    "    for i, (x, y) in enumerate(train_loader, start=1):\n",
    "        # move tensors to GPU/CPU as chosen earlier\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)   # clear old grads\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        if config[\"grad_clip\"] is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
    "\n",
    "        # parameter update\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        running += loss.item()\n",
    "        global_step += 1\n",
    "        if global_step % config[\"log_every\"] == 0:\n",
    "            avg_loss = running / config[\"log_every\"]\n",
    "            avg_bpc = bpc_from_loss(avg_loss)\n",
    "            print(f\"Epoch {epoch:02d} | Step {global_step:06d} \"\n",
    "                  f\"| train loss {avg_loss:.4f} | bpc {avg_bpc:.3f}\")\n",
    "            running = 0.0\n",
    "        \n",
    "        # sample generated text periodically\n",
    "        if global_step % config[\"sample_every\"] == 0:\n",
    "            print(\"Sample\")\n",
    "    \n",
    "    # end-of-epoch validation\n",
    "    val_loss, val_bpc = evaluate_loss(val_loader)\n",
    "    print(f\"[Validation] Epoch {epoch}: loss {val_loss:.4f} | bpc {val_bpc:.3f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"config\": config,\n",
    "            \"stoi\": vocab.stoi,\n",
    "            \"itos\": vocab.itos,\n",
    "        }, config[\"save_path\"])\n",
    "        print(f\"Saved checkpoint to {config['save_path']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded\n",
      "Sample\n"
     ]
    }
   ],
   "source": [
    "def load_model_checkpoint(path: str, rnn_type: str = None):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    itos = ckpt[\"itos\"]\n",
    "    stoi = ckpt[\"stoi\"]\n",
    "\n",
    "    # Rebuild vocab object\n",
    "    loaded_vocab = CharVocab(\"\".join(itos))\n",
    "    loaded_vocab.itos = itos\n",
    "    loaded_vocab.stoi = stoi\n",
    "\n",
    "    cfg = ckpt[\"config\"]\n",
    "    if rnn_type is not None:\n",
    "        cfg[\"rnn_type\"] = rnn_type\n",
    "\n",
    "    loaded_model = CharRNN(\n",
    "        vocab_size=len(itos),\n",
    "        emb=cfg[\"embedding_dim\"],\n",
    "        hidden=cfg[\"hidden_dim\"],\n",
    "        layers=cfg[\"num_layers\"],\n",
    "        dropout=cfg[\"dropout\"],\n",
    "        rnn_type=cfg[\"rnn_type\"],\n",
    "    ).to(device)\n",
    "    loaded_model.load_state_dict(ckpt[\"model_state\"])\n",
    "    loaded_model.eval()\n",
    "    return loaded_model, loaded_vocab, cfg\n",
    "\n",
    "if os.path.exists(config[\"save_path\"]):\n",
    "    loaded_model, loaded_vocab, loaded_cfg = load_model_checkpoint(config[\"save_path\"])\n",
    "    print(\"Checkpoint loaded\")\n",
    "    print(\"Sample\")\n",
    "else:\n",
    "    print(\"No checkpoint found yet. Train first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
