{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level RNN for Text Generation\n",
    "\n",
    "Train a small character-level language model on a single file to generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Config\n",
    "- imports, device, seed\n",
    "- simple config dictionary for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)                           # Python's random functions\n",
    "    torch.manual_seed(seed)                     # PyTorch CPU ops\n",
    "    torch.cuda.manual_seed_all(seed)            # PyTorch GPU ops\n",
    "    torch.backends.cudnn.deterministic = True   # cuDNN algortihm choice\n",
    "    torch.backends.cudnn.benchmark = False               # cuDNN auto-tuner\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"data_path\": None,      # path to text file (e.g., \"data/shakespeare.text\"); None uses built-in sample\n",
    "    \"seq_len\": 128,         # sequence length for training (input and target chunks)\n",
    "    \"batch_size\": 128,      # number of sequences per training batch\n",
    "    \"embedding_dim\": 256,   # size of character embedding vectors\n",
    "    \"hidden_dim\": 256,      # size of hidden state in RNN (GRU or LSTM)\n",
    "    \"num_layers\": 1,        # number of stacked RNN layers\n",
    "    \"dropout\": 0.1,         # dropout probability between layers\n",
    "    \"rnn_type\": \"GRU\",      # type of RNN: \"GRU\" or \"LSTM\"\n",
    "    \"num_epochs\": 5,        # number of full passes through the training dataset\n",
    "    \"learning_rate\": 2e-3,  # inital learning rate for the optimizer\n",
    "    \"grad_clip\": 1,         # gradient clipping threshold to prevent exploding gradients\n",
    "    \"log_every\": 100,       # how often (in steps) to print training loss\n",
    "    \"sample_every\": 100,    # how often (in steps) to generate sample text\n",
    "    \"max_generate\": 400,    # number of characters tto generate during sampling\n",
    "    \"temperature\": 0.9,     # sampling temperature (controls randomness in output)\n",
    "    \"top_k\": 40,            # top-k sampling: consider only the top k most probable characters\n",
    "    \"top_p\": 0.9,           # top-p (nucleus) sampling: consider top tokens whose probabilites sum to p\n",
    "    \"val_fraction\": 0.05,   # fraction of the dataset to use for validation\n",
    "    \"overlap_step\": None,   # if set, use overlapping training chunks (e.g, step size = seq_len // 2)\n",
    "    \"save_path\": \"char_rnn_checkpoint.pt\",  # where to save the trained model checkpoint\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
