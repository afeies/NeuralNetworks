{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance log\n",
    "1. 5 epochs: 2.173\n",
    "1. 20 epochs: 1.906\n",
    "    - cpu (Apple M4): 2 min 2.3 s\n",
    "    - mps: 2 min 12.6 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "[Validation] Epoch 1: loss 2.0277 | bpc 2.925\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 02 | Step 000100 | train loss 0.9211 | bpc 1.329\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO:;;.\n",
      "\n",
      "If her a cride, she stoid. Mary as some lis the prost siter and her proughter way canseal a mary, she dee of light said she said not\n",
      "in a pleatien at the cast the stamiting have storing, a compesself the had nore a coutitaintly to\n",
      "was an the lork, the\n",
      "stolad and mastine to the lass the was sarioth in the gating of her forming had buse of she deness. Wame the cried bore were to sard it was out\n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 2: loss 1.7766 | bpc 2.563\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 3: loss 1.6462 | bpc 2.375\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 04 | Step 000200 | train loss 0.6839 | bpc 0.987\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO:\n",
      "\n",
      "“Holt was in a little and his something to her that were carreathing of the work be may face the troming sectused as the a going to her. The eyes been one of chertain. And I was not of his had sure dift with his disting alones in it. I don't come in the talked Honora, “the man up not them at a little possity to it to as I had don't a grove by the had have look to her about the surig rook for an \n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 4: loss 1.5604 | bpc 2.251\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 5: loss 1.5085 | bpc 2.176\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 06 | Step 000300 | train loss 0.5405 | bpc 0.780\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO:\n",
      "\n",
      "“What was she strange, it was one of the conterst that you would not a great was discontinusing in the supposes which loved her gained her, and the hasting her but who mently the married, and comparise. The restanced the humber, doing the servely before the disting which she care who was sumperiate in summer the was as she was strove beside the come of his face that he said stood and possed the\n",
      "\n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 6: loss 1.4656 | bpc 2.114\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 7: loss 1.4404 | bpc 2.078\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 08 | Step 000400 | train loss 0.4267 | bpc 0.616\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO: No tain with a face she was such all the table, a pround and told you, in\n",
      "business, a dear eyes will be at the ortly, and there was like the good of the fired the destations will see the greet that anything for a family eyes to gazes, in the sense in the seatic sure as more she had took\n",
      "minding out that she fair face her only too reality for a thing remaining with her after a strainted that she f\n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 8: loss 1.4128 | bpc 2.038\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 9: loss 1.3985 | bpc 2.018\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 10 | Step 000500 | train loss 0.3288 | bpc 0.474\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO: Honora could not instant her about her husband. He presently, seemed a party believe at the man some that remarks about the gathered the breakfast and made still come as an early strept\n",
      "in the friends home to assief the can and landly in with herself to her and old could take a satime from the shared in her of the tall and anything in the country, and can give the sudden of both on the change for\n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 10: loss 1.3810 | bpc 1.992\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 11: loss 1.3753 | bpc 1.984\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 12 | Step 000600 | train loss 0.2370 | bpc 0.342\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO: Chiltern was to be the near one powers. He laughed.\n",
      "\n",
      "“I have got to can it is by a since some your in a little more more than I suppose I can't you will should have not exist, see the moments, the grass to was gravely. It is only been another imports of the minst with any before the carried a little extrived in the ground was on the enjoy that is before the stairs of the doors of his modered in h\n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 12: loss 1.3620 | bpc 1.965\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 13: loss 1.3571 | bpc 1.958\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 14 | Step 000700 | train loss 0.1511 | bpc 0.218\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO: Honora seemed to the night that she reputive that the best, or ressed the house. He laughed, when they had got it, and he said, when they had been in her strange felt. And in her lander of her windows, stock before and said of posite in the beal, the only whom they people was one she had gone his world who was she was to be coming at the stared the people period in its starting her instantly summ\n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 14: loss 1.3464 | bpc 1.942\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 15: loss 1.3453 | bpc 1.941\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 16 | Step 000800 | train loss 0.0675 | bpc 0.097\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO: CHAPTER VII IN WHLA\n",
      "\n",
      "Honora laughed and belonged something to the stranger by the time, with a place of a strange town, which the last, and who had been such a trees had he should not little dinner that she was at a wide of his remain the wild that Honora was such a wide protested.\n",
      "\n",
      "“There was the current to see them to think which she was likewise to her heavy sat had been power of the old as a \n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 16: loss 1.3352 | bpc 1.926\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 17 | Step 000900 | train loss 0.7010 | bpc 1.011\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO: She remarked, “that Mrs. Holt,” she said, and only continued against the card. With a seating she was only people who had forth such considered the porting despires. “And I should have got it,” she asked.\n",
      "\n",
      "“And it's she had are a long of a little a fear which I don't be only to be sure of one of his pollowed place of the modern ago and over that she thought of country, she could be a discussion. \n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 17: loss 1.3347 | bpc 1.926\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 18: loss 1.3384 | bpc 1.931\n",
      "Epoch 19 | Step 001000 | train loss 0.6121 | bpc 0.883\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO: How are not the famber that well knew the good began at Honora, who was alled by no subject of breakfast to get an actained the characterity as he did not approve the store her from the possession. There was not withered to sure from New York believed the sound in the things of the bright and life. And she thinking to conversaitive that was one of the happened to his preparing to the door and lat\n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 19: loss 1.3299 | bpc 1.919\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 20: loss 1.3303 | bpc 1.919\n"
     ]
    }
   ],
   "source": [
    "import math, os, random, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "config = {\n",
    "    \"data_path\": \"data/modern_chronicle.txt\",\n",
    "    \"seq_len\": 128,\n",
    "    \"batch_size\": 128,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 1,\n",
    "    \"dropout\": 0.1,\n",
    "    \"rnn_type\": \"GRU\",\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"log_every\": 100,\n",
    "    \"sample_every\": 100,\n",
    "    \"max_generate\": 400,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.9,\n",
    "    \"val_fraction\": 0.05,\n",
    "    \"overlap_step\": None,\n",
    "    \"save_path\": \"char_rnn_checkpoint.pt\"\n",
    "}\n",
    "\n",
    "if config[\"data_path\"] and os.path.exists(config[\"data_path\"]):\n",
    "    with open(config[\"data_path\"], \"r\", encoding=\"utf-8\") as f: text = f.read()\n",
    "else:\n",
    "    text = \"ROMEO:\\nBut soft, what light through yonder window breaks?\\nIt is the east, and Juliet is the sun.\\n\"\n",
    "\n",
    "class CharVocab:\n",
    "    def __init__(self, text):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.itos = chars\n",
    "        self.stoi = {c: i for i, c in enumerate(chars)}\n",
    "    \n",
    "    def encode(self, s):\n",
    "        return [self.stoi[c] for c in s if c in self.stoi]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.itos[i] for i in ids)\n",
    "\n",
    "vocab = CharVocab(text)\n",
    "vocab_size = len(vocab.itos)\n",
    "data_ids = torch.tensor(vocab.encode(text), dtype=torch.long)\n",
    "n_total = len(data_ids)\n",
    "n_val = max(1, int(n_total * config[\"val_fraction\"]))\n",
    "train_ids = data_ids[:-n_val]\n",
    "val_ids = data_ids[-n_val:]\n",
    "\n",
    "class CharChunkDataset(Dataset):\n",
    "    def __init__(self, ids, seq_len, step=None):\n",
    "        self.ids = ids\n",
    "        self.T = seq_len\n",
    "        self.step = step if step is not None else seq_len\n",
    "        self.num_chunks = (len(ids) - 1 - seq_len) // self.step + 1\n",
    "        self.starts = [i * self.step for i in range(self.num_chunks)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_chunks\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.starts[idx]\n",
    "        return self.ids[s:s + self.T], self.ids[s + 1:s + 1 + self.T]\n",
    "\n",
    "train_ds = CharChunkDataset(train_ids, config[\"seq_len\"], config[\"overlap_step\"])\n",
    "val_ds = CharChunkDataset(val_ids, config[\"seq_len\"], config[\"overlap_step\"])\n",
    "train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb, hidden, layers, dropout, rnn_type=\"GRU\"):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb)\n",
    "        rnn_cls = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[rnn_type.upper()]\n",
    "        self.rnn = rnn_cls(emb, hidden, num_layers=layers, dropout=dropout if layers > 1 else 0.0, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden, vocab_size)\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "        self.layers = layers\n",
    "        self.hidden = hidden\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        x = self.emb(x)\n",
    "        x, h = self.rnn(x, h)\n",
    "        x = self.drop(x)\n",
    "        return self.fc(x), h\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            return (torch.zeros(self.layers, batch_size, self.hidden, device=device),\n",
    "                    torch.zeros(self.layers, batch_size, self.hidden, device=device))\n",
    "        else:\n",
    "            return torch.zeros(self.layers, batch_size, self.hidden, device=device)\n",
    "\n",
    "model = CharRNN(vocab_size, config[\"embedding_dim\"], config[\"hidden_dim\"], config[\"num_layers\"], config[\"dropout\"], config[\"rnn_type\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "def bpc_from_loss(loss_val):\n",
    "    return loss_val / math.log(2.0)\n",
    "\n",
    "def evaluate_loss(data_loader):\n",
    "    model.eval(); total = 0.0; count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            total += loss.item(); count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    \n",
    "    avg = total / count\n",
    "    return avg, bpc_from_loss(avg)\n",
    "\n",
    "def sample_text(model, vocab, max_new_tokens=300, temperature=1.0, top_k=None, top_p=None, prompt=\"\", device=\"cpu\"):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if not prompt:\n",
    "            prompt = random.choice(vocab.itos)\n",
    "        \n",
    "        input_ids = torch.tensor(vocab.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        h = None\n",
    "        out = list(prompt)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, h = model(input_ids, h)\n",
    "            last_logits = logits[0, -1, :] / max(1e-6, temperature)\n",
    "            probs = torch.softmax(last_logits, dim=-1)\n",
    "            \n",
    "            if top_k is not None:\n",
    "                k = min(top_k, probs.numel())\n",
    "                topk_vals, topk_idx = torch.topk(probs, k)\n",
    "                mask = torch.zeros_like(probs, dtype=torch.bool); mask[topk_idx] = True\n",
    "                probs = probs.masked_fill(~mask, 0)\n",
    "            \n",
    "            if top_p is not None:\n",
    "                sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "                cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "                keep = cumsum <= top_p; keep[0] = True\n",
    "                filtered = torch.zeros_like(sorted_probs).masked_scatter(keep, sorted_probs[keep])\n",
    "                probs = torch.zeros_like(probs).scatter(0, sorted_idx, filtered)\n",
    "            \n",
    "            s = probs.sum()\n",
    "            \n",
    "            if s <= 0 or torch.isnan(s):\n",
    "                next_id = torch.argmax(last_logits)\n",
    "            \n",
    "            else:\n",
    "                probs = probs / s\n",
    "                next_id = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            out.append(vocab.itos[int(next_id)])\n",
    "            input_ids = torch.tensor([[next_id]], device=device)\n",
    "        \n",
    "        return \"\".join(out)\n",
    "\n",
    "global_step = 0; best_val = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader, start=1):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        if config[\"grad_clip\"] is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
    "        \n",
    "        optimizer.step()\n",
    "        running += loss.item(); global_step += 1\n",
    "        \n",
    "        if global_step % config[\"log_every\"] == 0:\n",
    "            avg_loss = running / config[\"log_every\"]\n",
    "            avg_bpc = bpc_from_loss(avg_loss)\n",
    "            print(f\"Epoch {epoch:02d} | Step {global_step:06d} | train loss {avg_loss:.4f} | bpc {avg_bpc:.3f}\")\n",
    "            running = 0.0\n",
    "        \n",
    "        if global_step % config[\"sample_every\"] == 0:\n",
    "            print(\"\\n--- Sample ---\")\n",
    "            print(sample_text(model, vocab, max_new_tokens=config[\"max_generate\"], temperature=config[\"temperature\"], top_k=config[\"top_k\"], top_p=config[\"top_p\"], prompt=\"ROMEO:\", device=device))\n",
    "            print(\"--------------\\n\")\n",
    "    \n",
    "    val_loss, val_bpc = evaluate_loss(val_loader)\n",
    "    print(f\"[Validation] Epoch {epoch}: loss {val_loss:.4f} | bpc {val_bpc:.3f}\")\n",
    "    \n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save({\"model_state\": model.state_dict(), \"config\": config, \"stoi\": vocab.stoi, \"itos\": vocab.itos}, config[\"save_path\"])\n",
    "        print(f\"Saved checkpoint to {config['save_path']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
