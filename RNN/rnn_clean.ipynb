{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Epoch 1: loss 2.0261 | bpc 2.923\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 02 | Step 000100 | train loss 0.9203 | bpc 1.328\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO:”!;, “We a for the was was inssing so the with you was me was of the retion the starain. She had that that when her she at had she coment her been that hat excensition the dorsistired she had her when she was suched on in said. “I bay suraing was ming, and calug seen that ser the ressed. And the was nought still seen in the shat\n",
      "him of her in the winking, Honora a for wo arter not crage of a sail \n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 2: loss 1.7737 | bpc 2.559\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 3: loss 1.6411 | bpc 2.368\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "Epoch 04 | Step 000200 | train loss 0.6837 | bpc 0.986\n",
      "\n",
      "--- Sample ---\n",
      "ROMEO:.. She said Honora sure. The sundost in the arriding and remissalishing of the for a worned in where dearion she had been such an onceling with it will on her that he lears who had her had she read the respapering about the walked for and the\n",
      "eroct and sand such at the face, who have no been her had the care to the recond which as the there into the work. And the repears. It was bent as he was exc\n",
      "--------------\n",
      "\n",
      "[Validation] Epoch 4: loss 1.5590 | bpc 2.249\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n",
      "[Validation] Epoch 5: loss 1.5059 | bpc 2.173\n",
      "Saved checkpoint to char_rnn_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "import math, os, random, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"data_path\": \"data/modern_chronicle.txt\",\n",
    "    \"seq_len\": 128,\n",
    "    \"batch_size\": 128,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 1,\n",
    "    \"dropout\": 0.1,\n",
    "    \"rnn_type\": \"GRU\",\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"log_every\": 100,\n",
    "    \"sample_every\": 100,\n",
    "    \"max_generate\": 400,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.9,\n",
    "    \"val_fraction\": 0.05,\n",
    "    \"overlap_step\": None,\n",
    "    \"save_path\": \"char_rnn_checkpoint.pt\"\n",
    "}\n",
    "\n",
    "if config[\"data_path\"] and os.path.exists(config[\"data_path\"]):\n",
    "    with open(config[\"data_path\"], \"r\", encoding=\"utf-8\") as f: text = f.read()\n",
    "else:\n",
    "    text = \"ROMEO:\\nBut soft, what light through yonder window breaks?\\nIt is the east, and Juliet is the sun.\\n\"\n",
    "\n",
    "class CharVocab:\n",
    "    def __init__(self, text):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.itos = chars\n",
    "        self.stoi = {c: i for i, c in enumerate(chars)}\n",
    "    \n",
    "    def encode(self, s):\n",
    "        return [self.stoi[c] for c in s if c in self.stoi]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.itos[i] for i in ids)\n",
    "\n",
    "vocab = CharVocab(text)\n",
    "vocab_size = len(vocab.itos)\n",
    "data_ids = torch.tensor(vocab.encode(text), dtype=torch.long)\n",
    "n_total = len(data_ids)\n",
    "n_val = max(1, int(n_total * config[\"val_fraction\"]))\n",
    "train_ids = data_ids[:-n_val]\n",
    "val_ids = data_ids[-n_val:]\n",
    "\n",
    "class CharChunkDataset(Dataset):\n",
    "    def __init__(self, ids, seq_len, step=None):\n",
    "        self.ids = ids\n",
    "        self.T = seq_len\n",
    "        self.step = step if step is not None else seq_len\n",
    "        self.num_chunks = (len(ids) - 1 - seq_len) // self.step + 1\n",
    "        self.starts = [i * self.step for i in range(self.num_chunks)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_chunks\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.starts[idx]\n",
    "        return self.ids[s:s + self.T], self.ids[s + 1:s + 1 + self.T]\n",
    "\n",
    "train_ds = CharChunkDataset(train_ids, config[\"seq_len\"], config[\"overlap_step\"])\n",
    "val_ds = CharChunkDataset(val_ids, config[\"seq_len\"], config[\"overlap_step\"])\n",
    "train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb, hidden, layers, dropout, rnn_type=\"GRU\"):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb)\n",
    "        rnn_cls = {\"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[rnn_type.upper()]\n",
    "        self.rnn = rnn_cls(emb, hidden, num_layers=layers, dropout=dropout if layers > 1 else 0.0, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden, vocab_size)\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "        self.layers = layers\n",
    "        self.hidden = hidden\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        x = self.emb(x)\n",
    "        x, h = self.rnn(x, h)\n",
    "        x = self.drop(x)\n",
    "        return self.fc(x), h\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            return (torch.zeros(self.layers, batch_size, self.hidden, device=device),\n",
    "                    torch.zeros(self.layers, batch_size, self.hidden, device=device))\n",
    "        else:\n",
    "            return torch.zeros(self.layers, batch_size, self.hidden, device=device)\n",
    "\n",
    "model = CharRNN(vocab_size, config[\"embedding_dim\"], config[\"hidden_dim\"], config[\"num_layers\"], config[\"dropout\"], config[\"rnn_type\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "def bpc_from_loss(loss_val):\n",
    "    return loss_val / math.log(2.0)\n",
    "\n",
    "def evaluate_loss(data_loader):\n",
    "    model.eval(); total = 0.0; count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            total += loss.item(); count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    \n",
    "    avg = total / count\n",
    "    return avg, bpc_from_loss(avg)\n",
    "\n",
    "def sample_text(model, vocab, max_new_tokens=300, temperature=1.0, top_k=None, top_p=None, prompt=\"\", device=\"cpu\"):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if not prompt:\n",
    "            prompt = random.choice(vocab.itos)\n",
    "        \n",
    "        input_ids = torch.tensor(vocab.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        h = None\n",
    "        out = list(prompt)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, h = model(input_ids, h)\n",
    "            last_logits = logits[0, -1, :] / max(1e-6, temperature)\n",
    "            probs = torch.softmax(last_logits, dim=-1)\n",
    "            \n",
    "            if top_k is not None:\n",
    "                k = min(top_k, probs.numel())\n",
    "                topk_vals, topk_idx = torch.topk(probs, k)\n",
    "                mask = torch.zeros_like(probs, dtype=torch.bool); mask[topk_idx] = True\n",
    "                probs = probs.masked_fill(~mask, 0)\n",
    "            \n",
    "            if top_p is not None:\n",
    "                sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "                cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "                keep = cumsum <= top_p; keep[0] = True\n",
    "                filtered = torch.zeros_like(sorted_probs).masked_scatter(keep, sorted_probs[keep])\n",
    "                probs = torch.zeros_like(probs).scatter(0, sorted_idx, filtered)\n",
    "            \n",
    "            s = probs.sum()\n",
    "            \n",
    "            if s <= 0 or torch.isnan(s):\n",
    "                next_id = torch.argmax(last_logits)\n",
    "            \n",
    "            else:\n",
    "                probs = probs / s\n",
    "                next_id = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            out.append(vocab.itos[int(next_id)])\n",
    "            input_ids = torch.tensor([[next_id]], device=device)\n",
    "        \n",
    "        return \"\".join(out)\n",
    "\n",
    "global_step = 0; best_val = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader, start=1):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        if config[\"grad_clip\"] is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
    "        \n",
    "        optimizer.step()\n",
    "        running += loss.item(); global_step += 1\n",
    "        \n",
    "        if global_step % config[\"log_every\"] == 0:\n",
    "            avg_loss = running / config[\"log_every\"]\n",
    "            avg_bpc = bpc_from_loss(avg_loss)\n",
    "            print(f\"Epoch {epoch:02d} | Step {global_step:06d} | train loss {avg_loss:.4f} | bpc {avg_bpc:.3f}\")\n",
    "            running = 0.0\n",
    "        \n",
    "        if global_step % config[\"sample_every\"] == 0:\n",
    "            print(\"\\n--- Sample ---\")\n",
    "            print(sample_text(model, vocab, max_new_tokens=config[\"max_generate\"], temperature=config[\"temperature\"], top_k=config[\"top_k\"], top_p=config[\"top_p\"], prompt=\"ROMEO:\", device=device))\n",
    "            print(\"--------------\\n\")\n",
    "    \n",
    "    val_loss, val_bpc = evaluate_loss(val_loader)\n",
    "    print(f\"[Validation] Epoch {epoch}: loss {val_loss:.4f} | bpc {val_bpc:.3f}\")\n",
    "    \n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save({\"model_state\": model.state_dict(), \"config\": config, \"stoi\": vocab.stoi, \"itos\": vocab.itos}, config[\"save_path\"])\n",
    "        print(f\"Saved checkpoint to {config['save_path']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
